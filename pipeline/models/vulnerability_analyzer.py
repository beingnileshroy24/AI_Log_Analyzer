import logging
import json
import os
from typing import Dict, Any
from dotenv import load_dotenv

load_dotenv()

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_openai import ChatOpenAI

class VulnerabilityAnalyzer:
    """
    Uses LLM to analyze vulnerabilities and generate solutions with reference URLs.
    Implements caching to reduce API costs.
    """
    
    def __init__(self, model_provider="google", model_name="gemini-2.5-flash"):
        self.llm = self._setup_llm(model_provider, model_name)
        self.cache = {}  # In-memory cache for solutions
        
    def _setup_llm(self, provider, model_name):
        """Initialize LLM based on provider."""
        try:
            if provider == "google":
                if not os.getenv("GOOGLE_API_KEY"):
                    logging.warning("âš ï¸ GOOGLE_API_KEY not found. Analyzer will use fallback solutions.")
                    return None
                return ChatGoogleGenerativeAI(model=model_name, temperature=0)
            elif provider == "openai":
                if not os.getenv("OPENAI_API_KEY"):
                    logging.warning("âš ï¸ OPENAI_API_KEY not found. Analyzer will use fallback solutions.")
                    return None
                return ChatOpenAI(model=model_name, temperature=0)
            else:
                return ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
        except Exception as e:
            logging.error(f"âŒ Failed to initialize LLM: {e}")
            return None
    
    def analyze_vulnerability(self, vuln_type: str, log_message: str) -> Dict[str, Any]:
        """
        Analyze a vulnerability and generate solution with reference URL.
        
        Args:
            vuln_type: Type of vulnerability (e.g., "SQL Injection (SQLi)")
            log_message: The actual log message containing the vulnerability
            
        Returns:
            Dict with keys: severity, solution, reference_url
        """
        # Check cache first
        cache_key = vuln_type
        if cache_key in self.cache:
            logging.info(f"ðŸ“¦ Using cached solution for {vuln_type}")
            return self.cache[cache_key]
        
        # If no LLM, use fallback
        if not self.llm:
            return self._get_fallback_solution(vuln_type)
        
        # Generate solution using LLM
        try:
            prompt = self._create_analysis_prompt(vuln_type, log_message)
            response = self.llm.invoke(prompt)
            result = self._parse_llm_response(response.content, vuln_type)
            
            # Cache the result
            self.cache[cache_key] = result
            logging.info(f"ðŸ¤– Generated LLM solution for {vuln_type}")
            
            return result
        except Exception as e:
            logging.error(f"âŒ LLM analysis failed for {vuln_type}: {e}")
            return self._get_fallback_solution(vuln_type)
    
    def _create_analysis_prompt(self, vuln_type: str, log_message: str) -> str:
        """Create a structured prompt for the LLM."""
        return f"""You are a cybersecurity expert. Analyze the following vulnerability and provide a solution.

Vulnerability Type: {vuln_type}
Log Message: {log_message}

Please provide your response in the following JSON format:
{{
    "severity": "High|Medium|Low",
    "solution": "Step-by-step solution to fix this vulnerability",
    "reference_url": "Official documentation or OWASP link for this vulnerability type"
}}

Be concise and practical. Focus on actionable steps."""
    
    def _parse_llm_response(self, response_text: str, vuln_type: str) -> Dict[str, Any]:
        """Parse LLM response and extract structured data."""
        try:
            # Try to extract JSON from response
            start_idx = response_text.find('{')
            end_idx = response_text.rfind('}') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = response_text[start_idx:end_idx]
                data = json.loads(json_str)
                
                return {
                    'severity': data.get('severity', 'Medium'),
                    'solution': data.get('solution', 'Review and sanitize input.'),
                    'reference_url': data.get('reference_url', self._get_default_reference(vuln_type))
                }
        except Exception as e:
            logging.warning(f"âš ï¸ Failed to parse LLM response: {e}")
        
        # Fallback if parsing fails
        return self._get_fallback_solution(vuln_type)
    
    def _get_fallback_solution(self, vuln_type: str) -> Dict[str, Any]:
        """Provide hardcoded solutions as fallback."""
        fallback_solutions = {
            "SQL Injection (SQLi)": {
                "severity": "High",
                "solution": "Use parameterized queries or prepared statements. Never concatenate user input directly into SQL queries. Implement input validation and use ORM frameworks.",
                "reference_url": "https://owasp.org/www-community/attacks/SQL_Injection"
            },
            "Cross-Site Scripting (XSS)": {
                "severity": "High",
                "solution": "Sanitize all user input. Use Content Security Policy (CSP). Encode output data. Validate input on both client and server side.",
                "reference_url": "https://owasp.org/www-community/attacks/xss/"
            },
            "Path Traversal (LFI/RFI)": {
                "severity": "High",
                "solution": "Validate and sanitize file paths. Use whitelisting for allowed files. Avoid using user input directly in file operations. Implement proper access controls.",
                "reference_url": "https://owasp.org/www-community/attacks/Path_Traversal"
            },
            "Brute Force / Auth Failure": {
                "severity": "Medium",
                "solution": "Implement rate limiting. Use account lockout mechanisms. Enable multi-factor authentication. Monitor failed login attempts. Use CAPTCHA after multiple failures.",
                "reference_url": "https://owasp.org/www-community/controls/Blocking_Brute_Force_Attacks"
            },
            "Command Injection": {
                "severity": "High",
                "solution": "Avoid using shell commands with user input. Use language-specific APIs instead. Implement strict input validation. Use whitelisting for allowed commands.",
                "reference_url": "https://owasp.org/www-community/attacks/Command_Injection"
            }
        }
        
        return fallback_solutions.get(vuln_type, {
            "severity": "Medium",
            "solution": "Review the log entry and apply security best practices. Consult OWASP guidelines for specific vulnerability type.",
            "reference_url": "https://owasp.org/www-project-top-ten/"
        })
    
    def _get_default_reference(self, vuln_type: str) -> str:
        """Get default reference URL for vulnerability type."""
        references = {
            "SQL Injection (SQLi)": "https://owasp.org/www-community/attacks/SQL_Injection",
            "Cross-Site Scripting (XSS)": "https://owasp.org/www-community/attacks/xss/",
            "Path Traversal (LFI/RFI)": "https://owasp.org/www-community/attacks/Path_Traversal",
            "Brute Force / Auth Failure": "https://owasp.org/www-community/controls/Blocking_Brute_Force_Attacks",
            "Command Injection": "https://owasp.org/www-community/attacks/Command_Injection"
        }
        return references.get(vuln_type, "https://owasp.org/www-project-top-ten/")

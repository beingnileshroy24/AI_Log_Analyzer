# AI Log Analyzer ğŸš€

An intelligent, modular log analysis pipeline that uses Machine Learning (TF-IDF, HDBSCAN) and Semantic Embeddings (Sentence-Transformers) to ingest, summarize, categorize, and cluster log files automatically. Now featuring an **Agentic RAG** mode for conversational log analysis, capable of performing advanced statistical and temporal analysis.

## ğŸŒŸ Key Features

* **Universal Ingestion**: Unified engine supporting `.log`, `.txt`, `.csv`, `.xlsx`, `.pdf`, `.parquet`, and API endpoints.
* **Intelligent Summarization**: Uses KeyBERT-style MMR (Maximal Marginal Relevance) to extract diverse and relevant keywords from large files.
* **Conversational AI Agent**: Built-in RAG (Retrieval-Augmented Generation) agent that lets you chat with your logs using Google Gemini.
* **Specialized Analysis Tools**:
  * **Statistics**: Detect and count duplicate log entries.
  * **Time Analysis**: Determine time range, duration, and peak activity hours.
  * **Pattern Matching**: Extract IP addresses, Emails, URLs, and Error Codes.
* **Hybrid Processing Modes**: Choose between file-level sorting (Best for organization) or line-level clustering (Best for pattern detection).
* **Metadata Auditing**: Automatically maintains a `file_master_report.csv` tracking the lifecycle of every file from ingestion to final destination.

---

## ğŸ“‚ Project Structure

The project has been reorganized into specialized submodules:

```text
AI_Log_Analyzer/
â”œâ”€â”€ main.py                     # Main Entry Point
â”œâ”€â”€ pipeline/                   # Core Logic Package
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py         # System Configuration and Keywords
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ ingestor.py         # Universal File Ingestion Engine
â”‚   â”‚   â”œâ”€â”€ metadata.py         # Audit Trail & CSV Reporting
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ summarizer.py       # Semantic Keyword Extraction (MMR)
â”‚   â”‚   â”œâ”€â”€ embedding.py        # Sentence-Transformer Integration
â”‚   â”‚   â”œâ”€â”€ rag_engine.py       # Vector DB & Retrieval
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py     # Batch Processing Logic ("Large Mode")
â”‚   â”‚   â”œâ”€â”€ clustering.py       # File Grouping Logic (HDBSCAN)
â”‚   â”‚   â”œâ”€â”€ processor.py        # Line-Level Clustering Engine ("Small Mode")
â”‚   â””â”€â”€ agent/
â”‚       â”œâ”€â”€ core.py             # Agent Logic
â”‚       â””â”€â”€ tools/              # Agent Capabilities
â”‚           â”œâ”€â”€ registry.py     # Tool Loader
â”‚           â”œâ”€â”€ stats_tool.py   # Dup Detection
â”‚           â”œâ”€â”€ time_tool.py    # Time Analysis
â”‚           â””â”€â”€ pattern_tool.py # Regex Extractor
â”œâ”€â”€ verification_scripts/       # System Health Checks
â”‚   â”œâ”€â”€ check_pipeline.py       # Python Import Checks
â”‚   â”œâ”€â”€ check_rag.py            # Vector DB Tests
â”‚   â”œâ”€â”€ check_agent.py          # Agent Tool Tests
â”‚   â””â”€â”€ check_llm.py            # API Connectivity Tests
â”œâ”€â”€ pipeline_data/              # Data persistence (Auto-generated)
â”œâ”€â”€ requirements.txt            # Dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration & Setup

1. **Clone the Repository**
2. **Create a Virtual Environment** (Recommended):
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Mac/Linux
   ```
3. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```
4. **Set up Environment Variables**:
   Create a `.env` file in the root directory and add your Google Gemini API key (required for Agent mode):
   ```ini
   GOOGLE_API_KEY=your_api_key_here
   ```

---

## ğŸš€ Usage

### 1. Prepare Data

Drop your log files or spreadsheets into the `pipeline_data/incoming/` folder.

### 2. Choose Your Mode

The AI Log Analyzer supports three primary modes:

#### **A. Large Mode (Default)**

*Focus: File-level categorization and sorting.*
Summarizes entire files and moves them to appropriate category folders.

```bash
python main.py large
```

#### **B. Small Mode**

*Focus: Line-level pattern detection within files.*
Clusters individual log lines to find common error patterns or event types across different files.

```bash
python main.py small
```

#### **C. Agent Mode (RAG)**

*Focus: Interactive Q&A.*
Chat with your processed logs to find specific errors, summaries, or insights.
**Note**: You must run "Large Mode" first to index the files.

```bash
python main.py agent
```

---

## ğŸ§ª Verification

This project comes with a dedicated suite of scripts to verify each component works correctly on your machine.

### Run All Checks

```bash
python verification_scripts/check_pipeline.py && \
python verification_scripts/check_rag.py && \
python verification_scripts/check_agent.py && \
python verification_scripts/check_llm.py
```

### Run Individual Checks

* **Pipeline Structure**: `python verification_scripts/check_pipeline.py`
* **Vector Database**: `python verification_scripts/check_rag.py`
* **Agent Tools**: `python verification_scripts/check_agent.py`
* **Gemini Connection**: `python verification_scripts/check_llm.py`

---

## ğŸ“Š Categories Supported

The AI automatically detects and sorts files into:

- **`app_log`**: API calls, HTTP logs, JSON responses, exceptions.
- **`system_log`**: Kernel logs, hardware metrics, server boot sequences.
- **`governance_log`**: Audit trails, compliance records, security policies.
- **`agreement`**: Legal documents, contracts, NDAs.
- **`unstructured_log`**: Generic or noisy logs.

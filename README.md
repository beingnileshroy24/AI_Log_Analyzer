# AI Log Analyzer ğŸš€

An intelligent, modular log analysis pipeline that uses Machine Learning (TF-IDF, HDBSCAN) and Semantic Embeddings (Sentence-Transformers) to ingest, summarize, categorize, and cluster log files automatically. Now featuring an **Agentic RAG** mode for conversational log analysis.

## ğŸŒŸ Key Features

*   **Universal Ingestion**: Unified engine supporting `.log`, `.txt`, `.csv`, `.xlsx`, `.pdf`, `.parquet`, and API endpoints.
*   **Intelligent Summarization**: Uses KeyBERT-style MMR (Maximal Marginal Relevance) to extract diverse and relevant keywords from large files.
*   **Conversational AI Agent**: Built-in RAG (Retrieval-Augmented Generation) agent that lets you chat with your logs using Google Gemini.
*   **Hybrid Processing Modes**: Choose between file-level sorting (Best for organization) or line-level clustering (Best for pattern detection).
*   **Metadata Auditing**: Automatically maintains a `file_master_report.csv` tracking the lifecycle of every file from ingestion to final destination.
*   **Automated Organization**: Physically moves files into categorized folders (`app_log`, `system_log`, `governance_log`, etc.) based on AI insights.

---

## ğŸ“‚ Project Structure

The project is organized into a modular package for better maintainability:

```text
AI_Log_Analyzer/
â”œâ”€â”€ main.py                # Main Entry Point
â”œâ”€â”€ pipeline/              # Core Logic Package
â”‚   â”œâ”€â”€ config.py          # System Configuration and Keywords
â”‚   â”œâ”€â”€ ingestor.py        # Universal File Ingestion Engine
â”‚   â”œâ”€â”€ summarizer.py      # Semantic Keyword Extraction (MMR)
â”‚   â”œâ”€â”€ embedding.py       # Sentence-Transformer Integration
â”‚   â”œâ”€â”€ file_clusterer.py  # HDBSCAN File Grouping Logic
â”‚   â”œâ”€â”€ processor.py       # Line-Level Clustering Engine
â”‚   â”œâ”€â”€ metadata.py        # Audit Trail & CSV Reporting
â”‚   â”œâ”€â”€ run_large_scale_pipeline.py  # Orchestrator for Large Mode
â”‚   â”œâ”€â”€ agent.py           # LangChain Agent & Tool Definitions
â”‚   â””â”€â”€ rag_engine.py      # ChromaDB Vector Store & Retrieval
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ verify_pipeline.py # Automated Test Suite
â”œâ”€â”€ pipeline_data/         # Data persistence (Auto-generated)
â”œâ”€â”€ requirements.txt       # Dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration & Setup

1.  **Clone the Repository**
2.  **Create a Virtual Environment** (Recommended):
    ```bash
    python -m venv .venv
    source .venv/bin/activate  # Mac/Linux
    ```
3.  **Install Dependencies**:
    ```bash
    pip install -r requirements.txt
    ```
4.  **Set up Environment Variables**:
    Create a `.env` file in the root directory and add your Google Gemini API key (required for Agent mode):
    ```ini
    GOOGLE_API_KEY=your_api_key_here
    ```

---

## ğŸš€ Usage

### 1. Prepare Data
Drop your log files or spreadsheets into the `pipeline_data/incoming/` folder.

### 2. Choose Your Mode
The AI Log Analyzer supports three primary modes:

#### **A. Large Mode (Default)**
*Focus: File-level categorization and sorting.*
Summarizes entire files and moves them to appropriate category folders.
```bash
python main.py large
```

#### **B. Small Mode**
*Focus: Line-level pattern detection within files.*
Clusters individual log lines to find common error patterns or event types across different files.
```bash
python main.py small
```

#### **C. Agent Mode (RAG)**
*Focus: Interactive Q&A.*
Chat with your processed logs to find specific errors, summaries, or insights.
**Note**: You must run "Large Mode" first to index the files.
```bash
python main.py agent
```

---

## ğŸ§ª Verification
You can run the automated verification script to ensure the pipeline is working correctly:
```bash
python scripts/verify_pipeline.py
```

---

## ğŸ“Š Categories Supported
The AI automatically detects and sorts files into:
- **`app_log`**: API calls, HTTP logs, JSON responses, exceptions.
- **`system_log`**: Kernel logs, hardware metrics, server boot sequences.
- **`governance_log`**: Audit trails, compliance records, security policies.
- **`agreement`**: Legal documents, contracts, NDAs.
- **`unstructured_log`**: Generic or noisy logs.

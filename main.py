import os
import shutil
import uuid
import logging
import sys

from config import (
    setup_directories,
    setup_logging,
    INCOMING_DIR,
    STAGING_DIR
)

from ingestor import UniversalIngestor
from metadata import generate_metadata_report, update_master_report
from processor import run_clustering

# Import Large Pipeline
try:
    from run_large_scale_pipeline import run_large_scale_pipeline
    LARGE_PIPELINE_AVAILABLE = True
except ImportError:
    LARGE_PIPELINE_AVAILABLE = False


def run_pipeline(mode="large"):
    """
    mode = "small"  -> line-level clustering (processor.py)
    mode = "large"  -> file-level summarization + sorting (run_large_scale_pipeline.py)
    """

    # 1Ô∏è‚É£ Setup
    setup_directories()
    setup_logging()
    logging.info(f"üöÄ Pipeline started in '{mode}' mode")

    # 2Ô∏è‚É£ Ingestion
    ingestor = UniversalIngestor(INCOMING_DIR)
    results = {}
    file_tracking = {}

    files_list = [f for f in os.listdir(INCOMING_DIR) 
                  if os.path.isfile(os.path.join(INCOMING_DIR, f))]

    if not files_list:
        logging.warning(f"‚ö†Ô∏è No files found in {INCOMING_DIR}.")
    
    for filename in files_list:
        source_path = os.path.join(INCOMING_DIR, filename)

        try:
            content = ingestor.process_file(source_path)
        except Exception as e:
            logging.error(f"‚ùå Ingestion failed for {filename}: {e}")
            continue

        if content is None:
            continue

        results[filename] = content

        # Rename + move to staging
        ext = os.path.splitext(filename)[1]
        new_name = f"{uuid.uuid4()}{ext}"
        dest_path = os.path.join(STAGING_DIR, new_name)

        shutil.move(source_path, dest_path)
        file_tracking[filename] = dest_path

        logging.info(f"‚úÖ Moved to staging: {new_name}")

    # 3Ô∏è‚É£ Initial Metadata (Status: Pending)
    if results:
        generate_metadata_report(results, file_tracking)

    # 4Ô∏è‚É£ Intelligence Layer
    if mode == "large":
        if not LARGE_PIPELINE_AVAILABLE:
            logging.error("‚ùå Large pipeline dependencies missing.")
            return
        
        # Run AI and get list of file movements
        updates = run_large_scale_pipeline()
        
        # 5Ô∏è‚É£ Finalize Metadata
        if updates:
            update_master_report(updates)
            
    else:
        logging.info("üß† Running LINE-LEVEL clustering")
        run_clustering(STAGING_DIR)

    logging.info("üèÅ Pipeline completed successfully")


if __name__ == "__main__":
    # Default to Large Scale (File Sorting) mode
    run_pipeline(mode="large")